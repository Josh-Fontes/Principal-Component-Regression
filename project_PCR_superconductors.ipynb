{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for general dataframe editing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for plotting\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# for pca and pcr\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from numpy.linalg import eig\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Setup and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "data = pd.read_csv('train.csv')\n",
    "data_elements = pd.read_csv('unique_m.csv')\n",
    "\n",
    "# look at dataset dimensions and first few observations\n",
    "print(data.shape)\n",
    "print(data_elements.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all the predictor variables\n",
    "predictors = ['number_of_elements', 'mean_atomic_mass', 'wtd_mean_atomic_mass',\n",
    "       'gmean_atomic_mass', 'wtd_gmean_atomic_mass', 'entropy_atomic_mass',\n",
    "       'wtd_entropy_atomic_mass', 'range_atomic_mass', 'wtd_range_atomic_mass',\n",
    "       'std_atomic_mass', 'wtd_std_atomic_mass', 'mean_fie', 'wtd_mean_fie',\n",
    "       'gmean_fie', 'wtd_gmean_fie', 'entropy_fie', 'wtd_entropy_fie',\n",
    "       'range_fie', 'wtd_range_fie', 'std_fie', 'wtd_std_fie',\n",
    "       'mean_atomic_radius', 'wtd_mean_atomic_radius', 'gmean_atomic_radius',\n",
    "       'wtd_gmean_atomic_radius', 'entropy_atomic_radius',\n",
    "       'wtd_entropy_atomic_radius', 'range_atomic_radius',\n",
    "       'wtd_range_atomic_radius', 'std_atomic_radius', 'wtd_std_atomic_radius',\n",
    "       'mean_Density', 'wtd_mean_Density', 'gmean_Density',\n",
    "       'wtd_gmean_Density', 'entropy_Density', 'wtd_entropy_Density',\n",
    "       'range_Density', 'wtd_range_Density', 'std_Density', 'wtd_std_Density',\n",
    "       'mean_ElectronAffinity', 'wtd_mean_ElectronAffinity',\n",
    "       'gmean_ElectronAffinity', 'wtd_gmean_ElectronAffinity',\n",
    "       'entropy_ElectronAffinity', 'wtd_entropy_ElectronAffinity',\n",
    "       'range_ElectronAffinity', 'wtd_range_ElectronAffinity',\n",
    "       'std_ElectronAffinity', 'wtd_std_ElectronAffinity', 'mean_FusionHeat',\n",
    "       'wtd_mean_FusionHeat', 'gmean_FusionHeat', 'wtd_gmean_FusionHeat',\n",
    "       'entropy_FusionHeat', 'wtd_entropy_FusionHeat', 'range_FusionHeat',\n",
    "       'wtd_range_FusionHeat', 'std_FusionHeat', 'wtd_std_FusionHeat',\n",
    "       'mean_ThermalConductivity', 'wtd_mean_ThermalConductivity',\n",
    "       'gmean_ThermalConductivity', 'wtd_gmean_ThermalConductivity',\n",
    "       'entropy_ThermalConductivity', 'wtd_entropy_ThermalConductivity',\n",
    "       'range_ThermalConductivity', 'wtd_range_ThermalConductivity',\n",
    "       'std_ThermalConductivity', 'wtd_std_ThermalConductivity',\n",
    "       'mean_Valence', 'wtd_mean_Valence', 'gmean_Valence',\n",
    "       'wtd_gmean_Valence', 'entropy_Valence', 'wtd_entropy_Valence',\n",
    "       'range_Valence', 'wtd_range_Valence', 'std_Valence', 'wtd_std_Valence']\n",
    "\n",
    "\n",
    "# making scatter plots for each predictor and the response \n",
    "plt.figure(figsize=(20,160))\n",
    "for i,j in enumerate(predictors):\n",
    "    plt.subplot(27,3,i+1)\n",
    "    plt.scatter(data[j],data[\"critical_temp\"], color = 'g', alpha = 0.5)\n",
    "    plt.title(predictors[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at distribution of response variable\n",
    "\n",
    "# setting plot style similar to R's ggplot\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# setting size of the plot\n",
    "plt.figure(figsize = (8,4))\n",
    "\n",
    "# plotting a histogram\n",
    "plt.hist(data['critical_temp'].values, bins = 20,\n",
    "        color = 'lightgreen',\n",
    "        edgecolor = 'k')\n",
    "plt.xlabel('Critical Temperature (K)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Critical Temperatures (K)')\n",
    "plt.xticks(np.arange(0, 200, step = 10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "corr_mat = data.corr()\n",
    "corr_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix heatmap\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.heatmap(corr_mat, cmap = 'viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIF to detect multicollinearity\n",
    "X = data.drop(['critical_temp'], axis = 1)\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF\"] = np.linalg.inv(X.corr()).diagonal()\n",
    "vif[\"features\"] = X.columns\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design matrix\n",
    "X = data[predictors].values\n",
    "\n",
    "# response vector\n",
    "y = data['critical_temp'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCA by hand to look at eigenstructure and explained ##\n",
    "\n",
    "# standardizing design matrix\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "# creating PCA object\n",
    "pca = PCA()\n",
    "\n",
    "# doing PCA to find principal components\n",
    "Z = pca.fit_transform(X_std)\n",
    "\n",
    "# creating principal components dataframe to look at\n",
    "Z_df = pd.DataFrame(data = Z, columns = list(range(1,82)))\n",
    "Z_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculating cumulative explained variance ##\n",
    "\n",
    "# getting eigenvalues and eigenvectors of the correlation matrix of the standardized design matrix\n",
    "E, V = eig(X_std.T @ X_std)\n",
    "print(E[0]/np.sum(E)) # explained variance for first principal component\n",
    "cumvar = np.cumsum(pca.explained_variance_ratio_) # cumulative explained variance for all principal components\n",
    "cumvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a plot for cumulative explained variance by the number of principal components\n",
    "print(cumvar[29]) # looking at cumulative explained variance for first 30 principal components\n",
    "plt.style.use('default') \n",
    "plt.figure(figsize = (25,10))\n",
    "plt.stem(cumvar, linefmt = 'lightgrey', markerfmt = 'navy', use_line_collection = True)\n",
    "plt.xticks(range(0,81), labels = list(range(1,82)))\n",
    "plt.xlabel('Number of Components', fontsize = 16)\n",
    "plt.ylabel('Cumulative Explained Variance', fontsize = 16);\n",
    "plt.title('Cumulative Explained Variance by Number of Components', fontsize = 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making scree plot - shows eigenvalues by principal components\n",
    "plt.style.use('default') \n",
    "plt.figure(figsize = (25,10))\n",
    "plt.plot(E)\n",
    "plt.xticks(range(0,81), labels = list(range(1,82)))\n",
    "plt.xlabel('Principal Component', fontsize = 16)\n",
    "plt.ylabel('Coresponding Eigenvalue', fontsize = 16);\n",
    "plt.title('Scree Plot', fontsize = 24);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to easily do PCR - source of code found in references section of paper\n",
    "\n",
    "def pcr(predictors, response, pc):\n",
    "    \n",
    "    ''' Step 1: PCA on input data'''\n",
    "    # Define the PCA object\n",
    "    pca = PCA()\n",
    "    \n",
    "    # Preprocess (2) Standardize features by removing the mean and scaling to unit variance\n",
    "    Xstd = StandardScaler().fit_transform(predictors)\n",
    "    # Run PCA producing the reduced variable Xred and select the first pc components\n",
    "    Z = pca.fit_transform(Xstd)[:,:pc]\n",
    "    ''' Step 2: regression on selected principal components'''\n",
    "    # Create linear regression object\n",
    "    regr = LinearRegression()\n",
    "    # Fit\n",
    "    Z_reg = regr.fit(Z, response)\n",
    "    # predicted values\n",
    "    y_hat = regr.predict(Z)\n",
    "    # Cross-validation\n",
    "    y_cv = cross_val_predict(Z_reg, Z, response, cv=10)\n",
    "    # Calculate scores for OG model and cross-validation models\n",
    "    R2 = r2_score(response, y_hat)\n",
    "    R2_cv = r2_score(response, y_cv)\n",
    "    # Calculate mean square error for OG model and cross validation models\n",
    "    mse = mean_squared_error(response, y_hat)\n",
    "    mse_cv = mean_squared_error(response, y_cv)\n",
    "    \n",
    "    ''' Step 3: Defining coefficients of Models'''\n",
    "    # coefficients of regressors for principal components regression\n",
    "    E, V = eig(Xstd.T @ Xstd)\n",
    "    Beta_Z = Z_reg.coef_\n",
    "    Beta_X = V[:,:pc] @ Beta_Z\n",
    "    \n",
    "    print('R2: %5.3f'  % R2)\n",
    "    print('R2 CV: %5.3f'  % R2_cv)\n",
    "    print('MSE: %5.3f' % mse)\n",
    "    print('MSE CV: %5.3f' % mse_cv)\n",
    "    print('Intercept:', Z_reg.intercept_)\n",
    "    print('Coefficients:', Z_reg.coef_)\n",
    "    return(Z_reg, Beta_X, y_hat, y_cv, R2, R2_cv, mse, mse_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the following several lines of code show PCR results for different subsets of principal components\n",
    "\n",
    "Z_model, beta_X, predicted, predicted_cv, r2, r2_cv, mse, mse_cv = pcr(X, y, pc = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_model, beta_X, predicted, predicted_cv, r2, r2_cv, mse, mse_cv = pcr(X, y, pc = 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_model, beta_X, predicted, predicted_cv, r2, r2_cv, mse, mse_cv = pcr(X, y, pc = 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_model, beta_X, predicted, predicted_cv, r2, r2_cv, mse, mse_cv = pcr(X, y, pc = 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_model, beta_X, predicted, predicted_cv, r2, r2_cv, mse, mse_cv = pcr(X, y, pc = 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_model, beta_X, predicted, predicted_cv, r2, r2_cv, mse, mse_cv = pcr(X, y, pc = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_model, beta_X, predicted, predicted_cv, r2, r2_cv, mse, mse_cv = pcr(X, y, pc = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to see what the coefficients look like after transformed back to standardized design matrix units\n",
    "print(Z_model.intercept_)\n",
    "print(beta_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for final model and for plotting\n",
    "Z_model, beta_X, predicted, predicted_cv, r2, r2_cv, mse, mse_cv = pcr(X, y, pc=30)\n",
    "\n",
    "\n",
    "# fitting a regression line for plotting\n",
    "z = np.polyfit(y, predicted, 1)\n",
    "\n",
    "# plotting the predicted values by the observed values to show how well the model predicted critical temperature and also\n",
    "# to plot the regression line\n",
    "with plt.style.context(('ggplot')):\n",
    "    fig, ax = plt.subplots(figsize = (9, 5))\n",
    "    ax.scatter(y, predicted, c = 'lightcyan', edgecolors = 'k')\n",
    "    ax.plot(y, y, c = 'yellow', linewidth = 2, label = 'Observed Tc') # can't make a dashed line for reasons I do not understand\n",
    "    ax.plot(y, z[1] + z[0]*y, c ='blue', linewidth = 2, label = 'Predicted Tc')\n",
    "    plt.title('Cross-validation R2: ' + str(round(r2_cv,3)))\n",
    "    plt.xlabel('Observed')\n",
    "    plt.ylabel('Predicted')\n",
    "    ax.legend(loc = 'upper left', facecolor = 'white');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function I either made myself or found (can't remember or find source) to find calculate five-number summary of a vector\n",
    "def summary_stats(data):\n",
    "\n",
    "    # calculate quartiles\n",
    "    quartiles = np.percentile(data, [25, 50, 75])\n",
    "    # calculate min/max\n",
    "    data_min, data_max = data.min(), data.max()\n",
    "    # print 5-number summary\n",
    "    print('Min: %.3f' % data_min)\n",
    "    print('Q1: %.3f' % quartiles[0])\n",
    "    print('Median: %.3f' % quartiles[1])\n",
    "    print('Q3: %.3f' % quartiles[2])\n",
    "    print('Max: %.3f' % data_max)\n",
    "\n",
    "\n",
    "# comparing five-number summaries between observed and predicted values\n",
    "summary_stats(predicted)\n",
    "summary_stats(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## plots for residual analysis ## \n",
    "  \n",
    "# setting plot style \n",
    "plt.style.use('ggplot') \n",
    "  \n",
    "# plotting residual errors \n",
    "plt.figure(figsize = (8,5))\n",
    "plt.scatter(predicted, y - predicted, \n",
    "            color = \"green\", s = 10)\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "\n",
    "# plot title \n",
    "plt.title(\"Residual Plot\", fontsize = 16); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qq-plot of residuals\n",
    "plt.figure(figsize = (8,5))\n",
    "stats.probplot(y - predicted, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Normal Q-Q Plot\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell contains code that was used to compare particular elements one at a time\n",
    "# notice that I did not drop the 'Oxygen' ('O') element from the dataset 'data' - want to use it for regression analysis and\n",
    "# to demonstrate using an indicator variable with PCR\n",
    "\n",
    "data_elements = pd.read_csv('unique_m.csv')\n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "data_elements = data_elements.drop(['critical_temp'], axis = 1)\n",
    "data = pd.concat([data, data_elements], axis = 1)\n",
    "data = data.drop(['H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'F', 'Ne','Na', \n",
    "                  'Mg', 'Al', 'Si', 'P', 'S', 'Cl','Ar', 'K','Ca', 'Sc', 'Ti', \n",
    "                  'V', 'Cr', 'Mn', 'Pb', 'Co','Ni', 'Cu', 'Zn', 'Ga', 'Ge', \n",
    "                  'As', 'Se', 'Br', 'Kr', 'Rb','Sr', 'Y', 'Zr', 'Nb', 'Mo', \n",
    "                  'Tc', 'Ru', 'Rh', 'Pd', 'Ag','Cd', 'In', 'Sn', 'Sb', 'Te', \n",
    "                  'I', 'Xe', 'Cs', 'Ba', 'La','Ce', 'Pr', 'Nd', 'Pm', 'Sm', \n",
    "                  'Eu', 'Gd', 'Tb', 'Dy', 'Ho','Er', 'Tm', 'Yb', 'Lu', 'Hf', \n",
    "                  'Ta', 'W', 'Re', 'Os', 'Ir','Pt', 'Au', 'Hg', 'Tl', 'Fe', \n",
    "                  'Bi', 'Po', 'At', 'Rn', 'material'], axis = 1)\n",
    "\n",
    "data_elements = data_elements[['H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne', 'Na', 'Mg', 'Al',\n",
    "       'Si', 'P', 'S', 'Cl', 'Ar', 'K', 'Ca', 'Sc', 'Ti', 'V', 'Cr', 'Mn',\n",
    "       'Fe', 'Co', 'Ni', 'Cu', 'Zn', 'Ga', 'Ge', 'As', 'Se', 'Br', 'Kr', 'Rb',\n",
    "       'Sr', 'Y', 'Zr', 'Nb', 'Mo', 'Tc', 'Ru', 'Rh', 'Pd', 'Ag', 'Cd', 'In',\n",
    "       'Sn', 'Sb', 'Te', 'I', 'Xe', 'Cs', 'Ba', 'La', 'Ce', 'Pr', 'Nd', 'Pm',\n",
    "       'Sm', 'Eu', 'Gd', 'Tb', 'Dy', 'Ho', 'Er', 'Tm', 'Yb', 'Lu', 'Hf', 'Ta',\n",
    "       'W', 'Re', 'Os', 'Ir', 'Pt', 'Au', 'Hg', 'Tl', 'Pb', 'Bi', 'Po', 'At',\n",
    "       'Rn']]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another five-number summary function (I definitely made this one myself) specifically to use on the elements dataset\n",
    "# I just wanted to get a sense of how many superconductors had which elements and how much of that element they had\n",
    "\n",
    "def summary_stats(df):\n",
    "    '''seven number summary of a numerical vector'''\n",
    "    global summary\n",
    "    names = ['Size','Min','25th Percentile','Median','75th Percentile','Max','Mean','Standard Deviation']\n",
    "    summary = pd.DataFrame(columns = names)\n",
    "    elements = []\n",
    "    for element in df.columns.values:\n",
    "        if df[element].values.max() != 0:\n",
    "            elements.append(element)\n",
    "            x = df[df[element].values > 0]\n",
    "            summaries = [len(x[element].values), round(x[element].values.min(),4), \n",
    "                         round(np.percentile(x[element].values, 25),4), \n",
    "                         round(np.percentile(x[element].values, 50),4), \n",
    "                         round(np.percentile(x[element].values, 75),4), \n",
    "                         round(x[element].values.max(),4), \n",
    "                         round(x[element].values.mean(),4), \n",
    "                         round(x[element].values.std(),4)]\n",
    "            summaries = pd.DataFrame([summaries], columns = names)\n",
    "            summary = summary.append(summaries)\n",
    "    summary = summary.set_index([elements])\n",
    "    return summary\n",
    "\n",
    "# it shows the number of super conductors that contain a particular element 'Size' along with the five-number summary\n",
    "# for instance, 299 superconductors contained Hydrogen and the superconductor that had the \n",
    "# most hydrogen had 14 (not sure about units)\n",
    "\n",
    "summary_stats(data_elements)\n",
    "summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# looking at all elements what were found in more than 2,000 superconductors\n",
    "# this is how I decided which elements to play with, eventually choosing Oxygen because of its clear PCA score plot\n",
    "summary[summary.Size > 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restructuring data to do PCA and eventually introduce indicator variable\n",
    "dataO = data[data['O'] > 0].reset_index()\n",
    "yO = dataO['critical_temp']\n",
    "dataO = dataO.drop(['index','critical_temp', 'O'], axis = 1)\n",
    "\n",
    "dataNO = data[data['O'] == 0].reset_index()\n",
    "yNO = dataNO['critical_temp']\n",
    "dataNO = dataNO.drop(['index','critical_temp', 'O'], axis = 1)\n",
    "\n",
    "y = pd.DataFrame(yNO)\n",
    "y = y.append(pd.DataFrame(data = yO), ignore_index = True)\n",
    "\n",
    "X_model = dataNO.append(dataO, ignore_index = True)\n",
    "\n",
    "# making oxygen into an indicator variable through factorization\n",
    "Oxygen0 = data['O'][data['O'] == 0].reset_index().drop(['index'], axis = 1)\n",
    "Oxygen1 = pd.DataFrame({'O': np.ones(11964)})\n",
    "\n",
    "Oxygen_edit = Oxygen0.append(Oxygen1, ignore_index = True)\n",
    "Oxygen = Oxygen_edit.apply(lambda x: x.factorize()[0])\n",
    "\n",
    "len(Oxygen[Oxygen['O'] == 1]) # correct size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCR by hand to carefully account for indicator variable ##\n",
    "\n",
    "# standardizing data\n",
    "X_std = StandardScaler().fit_transform(X_model)\n",
    "\n",
    "# creating PCA object\n",
    "pca = PCA()\n",
    "\n",
    "# principal components\n",
    "Z = pca.fit_transform(X_std)[:,:30]\n",
    "\n",
    "# creating principal components dataframe to look at\n",
    "Z_pca = pd.DataFrame(data = Z, columns = list(range(1,31)))\n",
    "\n",
    "# introducing indicator variable\n",
    "Z_df = pd.concat([Oxygen, Z_pca], axis = 1)\n",
    "\n",
    "# Create linear regression object\n",
    "regr = LinearRegression()\n",
    "\n",
    "# Fit\n",
    "Z_reg = regr.fit(Z_df, y)\n",
    "\n",
    "# predicted values\n",
    "y_hat = regr.predict(Z_df)\n",
    "\n",
    "# Cross-validation\n",
    "y_cv = cross_val_predict(Z_reg, Z_df, y, cv=10)\n",
    "\n",
    "# Calculate scores for OG model and cross-validation models\n",
    "R2 = r2_score(y, y_hat)\n",
    "R2_cv = r2_score(y, y_cv)\n",
    "\n",
    "# Calculate mean square error for OG model and cross validation models\n",
    "mse = mean_squared_error(y, y_hat)\n",
    "mse_cv = mean_squared_error(y, y_cv)\n",
    "    \n",
    "print('R2: %5.3f'  % R2)\n",
    "print('R2 CV: %5.3f'  % R2_cv)\n",
    "print('MSE: %5.3f' % mse)\n",
    "print('MSE CV: %5.3f' % mse_cv)\n",
    "print('Intercept:', Z_reg.intercept_)\n",
    "print('Coefficients:', Z_reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making observed values into a usable vector\n",
    "yvec = y['critical_temp']\n",
    "\n",
    "# making sure I subset the observed and predicted values correctly\n",
    "print(len(yvec[:9299])) # number of observations for super conductors without oxygen = 9299\n",
    "print(len(yvec[9299::])) # number of observations for super conductors with oxygen = 11964\n",
    "print(len(yvec[:9299]) + len(yvec[9299::])) # total observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fitting regression lines for plotting\n",
    "z0 = np.polyfit(yvec[:9299], y_hat[:9299], 1)\n",
    "z1 = np.polyfit(yvec[9299::], y_hat[9299::], 1)\n",
    "\n",
    "# plotting the predicted values by the observed values to show how well the model predicted critical temperature and also\n",
    "# to plot the regression lines\n",
    "with plt.style.context(('ggplot')):\n",
    "    fig, ax = plt.subplots(figsize = (10, 6))\n",
    "    ax.scatter(yvec[9299::], y_hat[9299::], c = 'lightcyan', edgecolors = 'k', label = 'With oxygen')\n",
    "    ax.scatter(yvec[:9299], y_hat[:9299], c = 'mistyrose', edgecolors = 'k', label= 'Without oxygen')\n",
    "    ax.plot(yvec, yvec, c ='yellow', linewidth = 2, label = 'Observed Tc') # still can't coerce a dashed line\n",
    "    ax.plot(yvec[:9299], z0[1] + z0[0]*yvec[:9299], c ='red', linewidth = 2, label = 'Without oxygen, predicted Tc')\n",
    "    ax.plot(yvec[9299::], z1[1] + z1[0]*yvec[9299::], c ='blue', linewidth = 2, label = 'With oxygen, predicted Tc')\n",
    "    plt.title('Cross-validation R2: ' + str(round(R2_cv,3)))\n",
    "    plt.xlabel('Observed')\n",
    "    plt.ylabel('Predicted')\n",
    "    ax.legend(loc = 'upper left', facecolor = 'white');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plots for residual analysis ##\n",
    "  \n",
    "# setting plot style \n",
    "plt.style.use('ggplot') \n",
    "  \n",
    "# plotting residual errors \n",
    "plt.figure(figsize = (8,5))\n",
    "plt.scatter(predicted, yvec - predicted, \n",
    "            color = \"green\", s = 10)\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "\n",
    "# plot title \n",
    "plt.title(\"Residual Plot\", fontsize = 16); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qq-plot of residuals\n",
    "plt.figure(figsize = (8,5))\n",
    "stats.probplot(yvec - predicted, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Normal Q-Q Plot\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
